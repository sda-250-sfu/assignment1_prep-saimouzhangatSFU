{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 \n",
    "\n",
    "Team members: Saimou Zhang, Amy Yang\n",
    "(If this file does not show the code properly, please visit SDA250 assignment1_prep-saimouzhangatSFU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 1: clinical studies\n",
    "Ear, Nose & Throat Journal  Retrieved from https://journals.sagepub.com/home/ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import corpus reader functionalities\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#Point to the path where you have some files\n",
    "#Change this for your own path\n",
    "corpus_root = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'ear_nose_throat_journal_clinicalstudies.txt',\n",
       " 'new_england_journal_of_medicine_perspectives.txt',\n",
       " 'news.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = PlaintextCorpusReader(corpus_root, '.*')\n",
    "texts.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accuracy', 'of', 'Age', '-', 'Based', 'Formula', ...]\n"
     ]
    }
   ],
   "source": [
    "# segment the texts in words\n",
    "ENTJ_words = texts.words('ear_nose_throat_journal_clinicalstudies.txt')\n",
    "print(ENTJ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6736"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of words \n",
    "len(ENTJ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to calculate the lexical diversity\n",
    "def lexical_diversity(nameOfSource):\n",
    "    lexdiv= len(nameOfSource)/len(set(nameOfSource))\n",
    "    return lexdiv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.969357690041249"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the lexical diversity \n",
    "lexical_diversity(ENTJ_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Accuracy', 'of', 'Age', '-', 'Based', 'Formula', 'to', 'Predict', 'the', 'Size', 'and', 'Depth', 'of', 'Cuffed', 'Oral', 'Preformed', 'Endotracheal', 'Tubes', 'in', 'Children', 'Undergoing', 'Tonsillectomy'], ['Introduction'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Segment the text into sentences \n",
    "ENTJ_sentences = texts.sents('ear_nose_throat_journal_clinicalstudies.txt')\n",
    "print(ENTJ_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "# Count the length of longest sentence\n",
    "ENTJ_longest_sentence_length = max(len(s) for s in ENTJ_sentences)\n",
    "print(ENTJ_longest_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['According', 'to', 'the', 'World', 'Health', 'Organization', '(', 'WHO', ')', 'growth', 'reference', 'criteria', 'for', 'body', 'mass', 'index', '(', 'BMI', ')', 'at', 'a', 'specific', 'age', ',', 'thinness', 'was', 'defined', 'as', 'BMI', '<', '\\x02', '2', 'SD', 'of', 'the', 'WHO', 'Growth', 'Reference', 'median', ',', 'normal', 'BMI', 'as', 'BMI', 'between', '\\x02', '2', 'SD', 'and', 'þ1', 'SD', ',', 'overweight', 'as', 'BMI', 'between', 'þ1', 'SD', 'and', 'þ2', 'SD', ',', 'and', 'obese', 'as', 'BMI', '>', 'þ2', 'SD', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sentence\n",
    "ENTJ_longest_sentence = [s for s in ENTJ_sentences if len(s) == ENTJ_longest_sentence_length]\n",
    "print(ENTJ_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accord\n",
      "to\n",
      "the\n",
      "world\n",
      "health\n",
      "organ\n",
      "(\n",
      "who\n",
      ")\n",
      "growth\n",
      "refer\n",
      "criteria\n",
      "for\n",
      "bodi\n",
      "mass\n",
      "index\n",
      "(\n",
      "bmi\n",
      ")\n",
      "at\n",
      "a\n",
      "specif\n",
      "age\n",
      ",\n",
      "thin\n",
      "wa\n",
      "defin\n",
      "as\n",
      "bmi\n",
      "<\n",
      "2\n",
      "SD\n",
      "of\n",
      "the\n",
      "whogrowth\n",
      "refer\n",
      "median\n",
      ",\n",
      "normal\n",
      "bmi\n",
      "as\n",
      "bmi\n",
      "between\n",
      "2\n",
      "SD\n",
      "and\n",
      "þ1\n",
      "SD\n",
      ",\n",
      "overweight\n",
      "as\n",
      "bmi\n",
      "between\n",
      "þ1\n",
      "SD\n",
      "and\n",
      "þ2\n",
      "SD\n",
      ",\n",
      "and\n",
      "obes\n",
      "as\n",
      "bmi\n",
      ">\n",
      "þ2\n",
      "SD\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stem the longest sentence \n",
    "ENTJ_longest_sentence_text = \"According to the World Health Organization (WHO)growth reference criteria for body mass index (BMI) at a specific age,thinness was defined as BMI < 2 SD of the WHOGrowth Reference median, normal BMI as BMI between 2 SD and þ1 SD, overweight as BMI between þ1 SD and þ2 SD, and obese as BMI > þ2 SD.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "porter = PorterStemmer()\n",
    "ENTJ_longest_sentence_tokenized = word_tokenize(ENTJ_longest_sentence_text)\n",
    "for t in ENTJ_longest_sentence_tokenized:\n",
    "    print(porter.stem(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petrous apex; costal cartilage; revision rhinoplasty; lateral crus;\n",
      "nasal valve; insertion depth; nasal obstruction; ETT size; CTM\n",
      "approach; cross-sectional area; inferior turbinate; Rose position;\n",
      "performed using; lateral crura; 22-Item Sinonasal; Obstruction\n",
      "Symptom; Symptom Evaluation; cavernous sinus; lacerum segment; visual\n",
      "analog\n"
     ]
    }
   ],
   "source": [
    "# find top collocations\n",
    "import nltk\n",
    "filePath1 = \"./data/ear_nose_throat_journal_clinicalstudies.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath1, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    ENTJ_data = f.read()\n",
    "    \n",
    "ENTJ_tokens = nltk.word_tokenize(ENTJ_data)\n",
    "ENTJ_text = nltk.Text(ENTJ_tokens)\n",
    "ENTJ_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "a\n",
      "as\n",
      "approach\n",
      "an\n",
      "at\n",
      "apex\n",
      "are\n",
      "age\n",
      "al\n",
      "approaches\n",
      "after\n",
      "assessed\n",
      "also\n",
      "airway\n",
      "actual\n",
      "accuracy\n",
      "all\n",
      "aged\n",
      "alar\n",
      "area\n",
      "around\n",
      "aesthetic\n",
      "anterior\n",
      "assess\n",
      "adequate\n",
      "administered\n",
      "air\n",
      "attempts\n",
      "according\n",
      "accurate\n",
      "analyses\n",
      "among\n",
      "ages\n",
      "appropriate\n",
      "assessment\n",
      "addition\n",
      "always\n",
      "airflow\n",
      "alteration\n",
      "and/or\n",
      "analog\n",
      "aid\n",
      "age-based\n",
      "avoid\n",
      "application\n",
      "anesthesia\n",
      "anesthesiologist\n",
      "although\n",
      "associated\n",
      "alternative\n",
      "above\n",
      "apply\n",
      "artery\n",
      "access\n",
      "abducens\n",
      "approximately\n",
      "angled\n",
      "anatomy\n",
      "allows\n",
      "affect\n",
      "author\n",
      "analysis\n",
      "autologous\n",
      "alone\n",
      "appropriately\n",
      "approval\n",
      "adenoidectomy\n",
      "audible\n",
      "aroused\n",
      "auscultated\n",
      "adjusted\n",
      "anticipated\n",
      "abnormal\n",
      "acceptable\n",
      "accordance\n",
      "affected\n",
      "assessable\n",
      "adolescents\n",
      "adult\n",
      "airways\n",
      "adiposity\n",
      "assume\n",
      "anatomic\n",
      "adjuvant\n",
      "antrostomy\n",
      "advantage\n",
      "advent\n",
      "anteromedial\n",
      "allow\n",
      "angle\n",
      "attack\n",
      "ability\n",
      "article\n",
      "academic\n",
      "adds\n",
      "attests\n",
      "advantages\n",
      "aims\n",
      "achieve\n",
      "acceleration\n",
      "array\n",
      "against\n",
      "affording\n",
      "aspects\n",
      "approved\n",
      "along\n",
      "absent\n",
      "available\n",
      "adherence\n",
      "across\n",
      "additional\n",
      "aim\n",
      "augmented\n",
      "assist\n",
      "aperture\n",
      "assessments\n",
      "again\n",
      "asked\n",
      "applied\n",
      "attached\n",
      "anesthetic\n",
      "analyze\n",
      "acts\n",
      "attachments\n",
      "anatomical\n",
      "already\n",
      "al3\n",
      "allocation\n",
      "able\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"a\". The top ten are \"and\", \"a\", \"as\", \"approach\",\"an\",\"at\",\"apex\",\"are\",\"age\",\"al\"\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "ENTJ_fdist = FreqDist(ENTJ_text)\n",
    "for w in ENTJ_fdist:\n",
    "    if w.startswith('a'):\n",
    "        print(w) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et\n",
      "each\n",
      "either\n",
      "endoscopic\n",
      "especially\n",
      "external\n",
      "endonasal\n",
      "ease\n",
      "exception\n",
      "efficacy\n",
      "ensure\n",
      "end\n",
      "expected\n",
      "experience\n",
      "extended\n",
      "effect\n",
      "effective\n",
      "extension\n",
      "experienced\n",
      "endoscopes\n",
      "effort\n",
      "edge\n",
      "excellent\n",
      "evaluating\n",
      "equal\n",
      "etiologies\n",
      "endotracheal\n",
      "electronic\n",
      "examined\n",
      "exclusion\n",
      "epinephrine\n",
      "error\n",
      "excluded\n",
      "except\n",
      "extubation\n",
      "endobronchial\n",
      "emergency\n",
      "even\n",
      "expertise\n",
      "embolism\n",
      "encephalomalacia\n",
      "examination\n",
      "erosive\n",
      "epistaxis\n",
      "epithelial\n",
      "extreme\n",
      "emission\n",
      "enhancement\n",
      "entire\n",
      "exposed\n",
      "encountered\n",
      "endoscope\n",
      "endoscopist\n",
      "exploration\n",
      "expanded\n",
      "examining\n",
      "extradural\n",
      "easily\n",
      "energy\n",
      "evaluate\n",
      "effectiveness\n",
      "endoscopy\n",
      "etiology\n",
      "every\n",
      "eighth\n",
      "established\n",
      "expressed\n",
      "emphasizes\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"e\", The ten ten are \"et\", \"each\", \"either\", \"endoscopic\", \"especially\", \"external\", \"endonasal\", \"easeexception\",\"efficacy\",\"ensure\"\n",
    "for w in ENTJ_fdist:\n",
    "    if w.startswith('e'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "is\n",
      "insertion\n",
      "inferior\n",
      "improved\n",
      "internal\n",
      "intubation\n",
      "its\n",
      "inspiratory\n",
      "into\n",
      "include\n",
      "improvement\n",
      "included\n",
      "infraorbital\n",
      "improving\n",
      "it\n",
      "intervention\n",
      "importance\n",
      "inflation\n",
      "increase\n",
      "inclusion\n",
      "intubated\n",
      "imaging\n",
      "incision\n",
      "including\n",
      "intraoperative\n",
      "institution\n",
      "intranasal\n",
      "inspiration\n",
      "inherent\n",
      "influx\n",
      "includes\n",
      "intravenous\n",
      "inhalation\n",
      "induction\n",
      "incisors\n",
      "indication\n",
      "ie\n",
      "inhaled\n",
      "index\n",
      "independent\n",
      "initially\n",
      "inflated\n",
      "incorrectly\n",
      "influenced\n",
      "intraoperatively\n",
      "inadvertent\n",
      "inconsistent\n",
      "individually\n",
      "inspection\n",
      "infarcts\n",
      "infection\n",
      "invasive\n",
      "isolated\n",
      "incomplete\n",
      "intensive\n",
      "intensity-modulated\n",
      "inside-out\n",
      "intercavernous\n",
      "identified\n",
      "injury\n",
      "indicated\n",
      "improve\n",
      "ipsilateral\n",
      "increases\n",
      "idiopathic\n",
      "ideal\n",
      "indicating\n",
      "investigation\n",
      "indications\n",
      "inferiorly\n",
      "intrinsic\n",
      "instability\n",
      "important\n",
      "inspired\n",
      "inward\n",
      "invisible\n",
      "increasing\n",
      "informed\n",
      "issues\n",
      "inserted\n",
      "instructed\n",
      "intricate\n",
      "integrated\n",
      "implemented\n",
      "initial\n",
      "interpretation\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"i\". The top ten are \"in\", \"is\", \"insertion\", \"inferior\", \"improved\", \"internal\", \"intubation\"\"its\", \"inspiratory\", \"into\"\n",
    "for w in ENTJ_fdist:\n",
    "    if w.startswith('i'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "or\n",
      "on\n",
      "obstruction\n",
      "objective\n",
      "outcomes\n",
      "our\n",
      "obese\n",
      "outcome\n",
      "only\n",
      "oral\n",
      "observed\n",
      "overweight\n",
      "over\n",
      "overall\n",
      "overnight\n",
      "one\n",
      "other\n",
      "open\n",
      "obviate\n",
      "obtained\n",
      "orientation\n",
      "ordinal\n",
      "overweight/obese\n",
      "out\n",
      "outer\n",
      "outward\n",
      "obtain\n",
      "overcome\n",
      "oroantral\n",
      "opening\n",
      "os\n",
      "opticocarotid\n",
      "overlying\n",
      "occurs\n",
      "offer\n",
      "opted\n",
      "optimal\n",
      "oxymetazoline\n",
      "option\n",
      "operating\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"o\". The top ten are \"of\"\"or\",\"on\",\"obstruction\",\"objective\",\"outcomes\",\"our\",\"obese\",\"outcome\",\"only\"\n",
    "for w in ENTJ_fdist:\n",
    "    if w.startswith('o'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using\n",
      "used\n",
      "underwent\n",
      "use\n",
      "undergoing\n",
      "underlay\n",
      "usually\n",
      "under\n",
      "up\n",
      "ultrasound\n",
      "unit\n",
      "upper\n",
      "unmentioned\n",
      "uncertain\n",
      "uncomplicated\n",
      "uninostril\n",
      "undermining\n",
      "ultimately\n",
      "undiagnosed\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"u\". The top ten are using,used,underwent,use,undergoing,underlay,usually,under,up,ultrasound\n",
    "for w in ENTJ_fdist:\n",
    "    if w.startswith('u'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 2: Perspective articles (published in domain-specific academic journal)\n",
    "New England Joural of Medicine Retrieved from https://www.nejm.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment the text into words\n",
    "NEJM_words = texts.words('new_england_journal_of_medicine_perspectives.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'Half', '-', 'Century', 'of', 'Progress', 'in', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEJM_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4991"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of words for NEJM.txt\n",
    "len(NEJM_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to calculate the lexical diversity\n",
    "def lexical_diversity(nameOfSource):\n",
    "    lexdiv= len(nameOfSource)/len(set(nameOfSource))\n",
    "    return lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3317757009345796"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the lexcial diversity of NEJM.txt\n",
    "lexical_diversity(NEJM_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A', 'Half', '-', 'Century', 'of', 'Progress', 'in', 'Health', ':', 'The', 'National', 'Academy', 'of', 'Medicine', 'at', '50'], ['Vaccine', 'Innovations', '—', 'Past', 'and', 'Future'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Segment sentences from the three texts\n",
    "NEJM_sentences = texts.sents('new_england_journal_of_medicine_perspectives.txt')\n",
    "print(NEJM_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "# Count the length of longest sentence in NEJM.txt\n",
    "NEJM__longest_sentence_length = max(len(s) for s in NEJM_sentences)\n",
    "print(NEJM__longest_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Other', 'information', 'gaps', 'will', 'include', 'more', 'comprehensive', 'assessments', 'of', 'short', '-', 'term', 'safety', ',', 'knowledge', 'of', 'whether', 'waning', 'of', 'vaccineinduced', 'protection', 'may', 'lead', 'to', 'vaccine', '-', 'enhanced', 'disease', 'if', 'a', 'vaccinee', 'becomes', 'infected', 'on', 'exposure', 'to', 'SARS', '-', 'CoV', '-', '2', ',', 'information', 'on', 'protection', 'against', 'clinically', 'severe', 'forms', 'of', 'Covid', '-', '19', ',', 'and', 'knowledge', 'of', 'any', 'associations', 'between', 'the', 'degree', 'of', 'protection', 'and', 'the', 'recipient', '’', 's', 'age', 'or', 'coexisting', 'conditions', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sentence in NEJM.txt\n",
    "NEJM_longest_sentence = [s for s in NEJM_sentences if len(s) == NEJM__longest_sentence_length]\n",
    "print(NEJM_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other\n",
      "inform\n",
      "gap\n",
      "will\n",
      "includ\n",
      "more\n",
      "comprehens\n",
      "assess\n",
      "of\n",
      "short-term\n",
      "safeti\n",
      ",\n",
      "knowledg\n",
      "of\n",
      "whether\n",
      "wane\n",
      "of\n",
      "vaccineinduc\n",
      "protect\n",
      "may\n",
      "lead\n",
      "to\n",
      "vaccine-enhanc\n",
      "diseas\n",
      "if\n",
      "a\n",
      "vaccine\n",
      "becom\n",
      "infect\n",
      "on\n",
      "exposur\n",
      "to\n",
      "sars-cov-2\n",
      ",\n",
      "inform\n",
      "on\n",
      "protect\n",
      "against\n",
      "clinic\n",
      "sever\n",
      "form\n",
      "of\n",
      "covid-19\n",
      ",\n",
      "and\n",
      "knowledg\n",
      "of\n",
      "ani\n",
      "associ\n",
      "between\n",
      "the\n",
      "degre\n",
      "of\n",
      "protect\n",
      "and\n",
      "the\n",
      "recipi\n",
      "’\n",
      "s\n",
      "age\n",
      "or\n",
      "coexist\n",
      "condit\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stem the longest sentence for NEJM.txt\n",
    "NEJM_longest_sentence_text = \"Other information gaps will include more comprehensive assessments of short-term safety, knowledge of whether waning of vaccineinduced protection may lead to vaccine-enhanced disease if a vaccinee becomes infected on exposure to SARS-CoV-2, information on protection against clinically severe forms of Covid-19, and knowledge of any associations between the degree of protection and the recipient’s age or coexisting conditions.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "porter = PorterStemmer()\n",
    "NEJM_longest_sentence_tokenized = word_tokenize(NEJM_longest_sentence_text)\n",
    "for t in NEJM_longest_sentence_tokenized:\n",
    "    print(porter.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public health; United States; clinical laboratories; placebo\n",
      "recipients; adverse events; infectious diseases; regulatory processes;\n",
      "blinded follow-up; National Academy; Placebo-Controlled Trials;\n",
      "navigate regulations; emergency use; Health Organization; World\n",
      "Health; Disease Control; decision making; placebo-controlled trials;\n",
      "immune system; safety concerns; surveillance testing\n"
     ]
    }
   ],
   "source": [
    "# find the top collocations \n",
    "import nltk\n",
    "filePath2 = \"./data/new_england_journal_of_medicine_perspectives.txt\"\n",
    "\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath2, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    NEJM_data = f.read()\n",
    "    \n",
    "NEJM_tokens = nltk.word_tokenize(NEJM_data)\n",
    "NEJM_text = nltk.Text(NEJM_tokens)\n",
    "NEJM_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "a\n",
      "are\n",
      "as\n",
      "an\n",
      "at\n",
      "about\n",
      "access\n",
      "available\n",
      "after\n",
      "against\n",
      "all\n",
      "any\n",
      "also\n",
      "among\n",
      "according\n",
      "associated\n",
      "address\n",
      "adverse\n",
      "appropriate\n",
      "addition\n",
      "approval\n",
      "administered\n",
      "approved\n",
      "administration\n",
      "additional\n",
      "asymptomatic\n",
      "allocation\n",
      "affordable\n",
      "authorized\n",
      "age\n",
      "assessments\n",
      "administer\n",
      "areas\n",
      "aren\n",
      "antibody\n",
      "attenuated\n",
      "assignments\n",
      "answers\n",
      "assigned\n",
      "already\n",
      "acceptance\n",
      "alone\n",
      "ages\n",
      "achieving\n",
      "ago\n",
      "authoritative\n",
      "achievement\n",
      "attributable\n",
      "accomplishment\n",
      "announced\n",
      "achievements\n",
      "adults\n",
      "advances\n",
      "alarms\n",
      "assess\n",
      "autism\n",
      "accessible\n",
      "always\n",
      "availability\n",
      "amplify\n",
      "agents\n",
      "appear\n",
      "airborne\n",
      "animals\n",
      "able\n",
      "antibodies\n",
      "aims\n",
      "academic\n",
      "audio\n",
      "announcements\n",
      "appears\n",
      "associations\n",
      "ad\n",
      "assign\n",
      "altruistic\n",
      "amenable\n",
      "attack\n",
      "analyzed\n",
      "attributed\n",
      "anecdotes\n",
      "appropriately\n",
      "assessment\n",
      "accompanied\n",
      "allocating\n",
      "attempts\n",
      "ascertain\n",
      "accelerate\n",
      "adequate\n",
      "assessing\n",
      "authorities\n",
      "achieve\n",
      "average\n",
      "asking\n",
      "answer\n",
      "authorizations\n",
      "allow\n",
      "added\n",
      "absence\n",
      "acid\n",
      "adaptable\n",
      "adequately\n",
      "allotted\n",
      "at-risk\n",
      "authorization\n",
      "altogether\n",
      "agency\n",
      "applicants\n",
      "allowing\n",
      "acquire\n",
      "authority\n",
      "amplification\n",
      "alternatives\n",
      "arrives\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"a\". The ten top words include and, a, are, as, an, at, about, access, available, after\n",
    "from nltk import FreqDist\n",
    "NEJM_fdist = FreqDist(NEJM_text)\n",
    "for w in NEJM_fdist:\n",
    "    if w.startswith('a'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective\n",
      "efficacy\n",
      "evidence\n",
      "early\n",
      "events\n",
      "even\n",
      "estimated\n",
      "emergency\n",
      "ethically\n",
      "existing\n",
      "encourage\n",
      "ensure\n",
      "effects\n",
      "emerge\n",
      "efforts\n",
      "example\n",
      "either\n",
      "enough\n",
      "enable\n",
      "eradication\n",
      "effort\n",
      "enhanced\n",
      "ethical\n",
      "essential\n",
      "evaluation\n",
      "exposure\n",
      "evaluate\n",
      "eventually\n",
      "efficiency\n",
      "equipment\n",
      "enhance\n",
      "evidence-based\n",
      "exceeded\n",
      "eliminated\n",
      "executive\n",
      "eradicated\n",
      "eligible\n",
      "evaluated\n",
      "evolved\n",
      "ecosystem\n",
      "exist\n",
      "epidemics\n",
      "epidemic\n",
      "experimental\n",
      "envelope\n",
      "establishing\n",
      "emerging\n",
      "everyone\n",
      "emerged\n",
      "enroll\n",
      "elucidate\n",
      "enhancing\n",
      "each\n",
      "ensuring\n",
      "explained\n",
      "exaggerating\n",
      "especially\n",
      "elsewhere\n",
      "efficient\n",
      "evaluating\n",
      "earning\n",
      "end\n",
      "enter\n",
      "exceeds\n",
      "establish\n",
      "easily\n",
      "expansion\n",
      "expenses\n",
      "e.g.\n",
      "expertise\n",
      "ease\n",
      "emergent\n",
      "empower\n",
      "expectations\n",
      "established\n",
      "engineered\n",
      "entity\n",
      "equitable\n",
      "economic\n",
      "easy-to-use\n",
      "explored\n",
      "enables\n",
      "emergence\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"e\". The ten top words include effective, efficacy, evidence, early, events, even, estimated, emergency, ethically, existing, encourage\n",
    "for w in NEJM_fdist:\n",
    "    if w.startswith('e'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "is\n",
      "important\n",
      "if\n",
      "it\n",
      "including\n",
      "investigational\n",
      "information\n",
      "its\n",
      "include\n",
      "infectious\n",
      "improve\n",
      "incentives\n",
      "innovative\n",
      "immunization\n",
      "infection\n",
      "influenza\n",
      "into\n",
      "incidence\n",
      "included\n",
      "increase\n",
      "isn\n",
      "immune\n",
      "interactions\n",
      "infected\n",
      "identified\n",
      "identify\n",
      "identifying\n",
      "institute\n",
      "illustrates\n",
      "inception\n",
      "issues\n",
      "illnesses\n",
      "immunization-related\n",
      "invasive\n",
      "inf\n",
      "issued\n",
      "improvements\n",
      "international\n",
      "imperative\n",
      "itself\n",
      "independent\n",
      "increasingly\n",
      "infections\n",
      "identification\n",
      "institutions\n",
      "inactivated\n",
      "interview\n",
      "improving\n",
      "insights\n",
      "initial\n",
      "increasing\n",
      "inform\n",
      "interpretation\n",
      "incorrectly\n",
      "immediate\n",
      "intervention\n",
      "instead\n",
      "informed\n",
      "impact\n",
      "inevitably\n",
      "immediately\n",
      "initially\n",
      "implementation\n",
      "ideally\n",
      "instances\n",
      "invocation\n",
      "ineffective\n",
      "infrastructure\n",
      "implement\n",
      "investigation\n",
      "isothermal\n",
      "innovation\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"i\".The ten top words include in, is, important, if, it, including, investigational, information, its, include\n",
    "for w in NEJM_fdist:\n",
    "    if w.startswith('i'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "on\n",
      "or\n",
      "other\n",
      "outbreak\n",
      "only\n",
      "obtain\n",
      "our\n",
      "ongoing\n",
      "own\n",
      "others\n",
      "one\n",
      "organizations\n",
      "oversight\n",
      "opportunity\n",
      "observational\n",
      "often\n",
      "ones\n",
      "overstate\n",
      "outcome\n",
      "objective\n",
      "outbreaks\n",
      "optimal\n",
      "opportunities\n",
      "optimization\n",
      "obligated\n",
      "obtained\n",
      "occur\n",
      "opposed\n",
      "option\n",
      "outcomes\n",
      "offer\n",
      "opened\n",
      "otherwise-unfettered\n",
      "outset\n",
      "overcame\n",
      "outside\n",
      "operates\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"o\".The top ten words include of, on, or, other, outbreak, only, obtain, our, ongoing, own\n",
    "for w in NEJM_fdist:\n",
    "    if w.startswith('o'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\n",
      "under\n",
      "used\n",
      "using\n",
      "until\n",
      "up\n",
      "understanding\n",
      "us\n",
      "usually\n",
      "unprecedented\n",
      "users\n",
      "unknown\n",
      "unblind\n",
      "understand\n",
      "unreliable\n",
      "unambiguous\n",
      "unvaccinated\n",
      "unrelated\n",
      "unbiased\n",
      "unblinding\n",
      "unexpected\n",
      "unambiguously\n",
      "ultimate\n",
      "uncommon\n",
      "useful\n",
      "updates\n",
      "unmet\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"u\". The top ten words include use, under, used, using, until, up, understanding, us, usually, unprecedented\n",
    "for w in NEJM_fdist:\n",
    "    if w.startswith('u'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 3: News articles (from various news channels)\n",
    "\n",
    "Retrieved from:\n",
    "\n",
    "Free exchange: Apps and downsides. (2016 November 5). The Economist. Retrieved from \n",
    "https://www.economist.com/finance-and-economics/2016/11/05/apps-and-downsides?fsrc=sponsor%2Fopenandsecure\n",
    "Mangione, K. (2021 February 9). Here’s how much you’d have to earn to buy a house or condo \n",
    "in Vancouver, according to a study. CTV News. Retrieved from \n",
    "https://bc.ctvnews.ca/here-s-how-much-you-d-have-to-earn-to-buy-a-house-or-condo-in-vancouver-according-to-a-study-1.5302233\n",
    "Martins, N. (2021 February 9). B.C. records another 435 cases of COVID-19, four more deaths. \n",
    "News 1130. Retrieved from https://www.citynews1130.com/2021/02/09/bc-covid-435-cases-four-deaths-tuesday/\n",
    "Nicholson, B. (2021 February 9). Only 12% of COVID-19 fines paid in B.C. while majority \n",
    "dispute ticket. News 1130. Retrieved from https://www.citynews1130.com/2021/02/09/12-percent-covid-fines-paid-bc/\n",
    "Why everything is hackable: Computer security is broken from top to bottom. (2017 April 8). \n",
    "The Economist. Retrieved from https://www.economist.com/science-and-technology/2017/04/08/computer-security-is-broken-from-top-to-bottom?fsrc=sponsor%2Fopenandsecure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import corpus reader functionalities\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#Point to the path where you have some files\n",
    "#Change this for your own path\n",
    "corpus_root = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'ear_nose_throat_journal_clinicalstudies.txt',\n",
       " 'new_england_journal_of_medicine_perspectives.txt',\n",
       " 'news.txt']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = PlaintextCorpusReader(corpus_root, '.*')\n",
    "texts.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apps', 'and', 'downsides', '“', 'Gig', '-', ...]\n"
     ]
    }
   ],
   "source": [
    "# segment the text into words\n",
    "news_words = texts.words('news.txt')\n",
    "print(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6345"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of words\n",
    "len(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to calculate the lexical diversity\n",
    "def lexical_diversity(nameOfSource):\n",
    "    lexdiv= len(nameOfSource)/len(set(nameOfSource))\n",
    "    return lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.440889370932755"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the lexcial diversity \n",
    "lexical_diversity(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apps', 'and', 'downsides', '“', 'Gig', '-', 'economy', '”', 'work', 'sits', 'outside', 'normal', 'employment', 'categories', 'DURING', 'a', 'recent', 'ride', 'with', 'Uber', ',', 'this', 'passenger', 'received', 'a', 'surprising', 'word', 'of', 'thanks', 'for', 'talking', 'softly', '.'], ['To', 'complete', 'the', 'job', ',', 'the', 'driver', 'needed', 'to', 'follow', 'the', 'route', 'provided', 'by', 'Uber', ',', 'read', 'out', 'turn', '-', 'by', '-', 'turn', 'by', 'his', 'phone', ';', 'noise', 'from', 'the', 'back', 'seat', 'drowned', 'out', 'the', 'critical', 'instructions', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Segment the text into sentences \n",
    "news_sentences = texts.sents('news.txt')\n",
    "print(news_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# Count the length of longest sentence \n",
    "news__longest_sentence_length = max(len(s) for s in news_sentences)\n",
    "print(news__longest_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Kenneth', 'White', ',', 'a', 'cryptography', 'researcher', 'in', 'Washington', ',', 'DC', ',', 'warns', 'that', 'if', 'the', 'government', 'comes', 'down', 'too', 'hard', ',', 'the', 'software', 'business', 'may', 'end', 'up', 'looking', 'like', 'the', 'pharmaceutical', 'industry', ',', 'where', 'tough', ',', 'ubiquitous', 'regulation', 'is', 'one', 'reason', 'why', 'the', 'cost', 'of', 'developing', 'a', 'new', 'drug', 'is', 'now', 'close', 'to', 'a', 'billion', 'dollars', '(', '50', ').']]\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sentence \n",
    "news_longest_sentence = [s for s in news_sentences if len(s) == news__longest_sentence_length]\n",
    "print(news_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kenneth\n",
      "white\n",
      ",\n",
      "a\n",
      "cryptographi\n",
      "research\n",
      "in\n",
      "washington\n",
      ",\n",
      "DC\n",
      ",\n",
      "warn\n",
      "that\n",
      "if\n",
      "the\n",
      "govern\n",
      "come\n",
      "down\n",
      "too\n",
      "hard\n",
      ",\n",
      "the\n",
      "softwar\n",
      "busi\n",
      "may\n",
      "end\n",
      "up\n",
      "look\n",
      "like\n",
      "the\n",
      "pharmaceut\n",
      "industri\n",
      ",\n",
      "where\n",
      "tough\n",
      ",\n",
      "ubiquit\n",
      "regul\n",
      "is\n",
      "one\n",
      "reason\n",
      "whi\n",
      "the\n",
      "cost\n",
      "of\n",
      "develop\n",
      "a\n",
      "new\n",
      "drug\n",
      "is\n",
      "now\n",
      "close\n",
      "to\n",
      "a\n",
      "billion\n",
      "dollar\n",
      "(\n",
      "50\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stem the longest sentence \n",
    "news_longest_sentence_text = \"Kenneth White, a cryptography researcher in Washington, DC, warns that if the government comes down too hard, the software business may end up looking like the pharmaceutical industry, where tough, ubiquitous regulation is one reason why the cost of developing a new drug is now close to a billion dollars (50).\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "porter = PorterStemmer()\n",
    "news_longest_sentence_tokenized = word_tokenize(news_longest_sentence_text)\n",
    "for t in news_longest_sentence_tokenized:\n",
    "    print(porter.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per cent; operating system; household income; credit-card details;\n",
      "minimum wage; could expect; NEWS 1130; electricity meters; national\n",
      "level; median income; economic incentives; medical devices; outside\n",
      "normal; National Bank; light bulbs; control Uber; labour markets;\n",
      "hacker God; mortgage payment; would need\n"
     ]
    }
   ],
   "source": [
    "# find the top collocations \n",
    "filePath3 = \"./data/news.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath3, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    news_data = f.read()\n",
    "    \n",
    "news_tokens = nltk.word_tokenize(news_data)\n",
    "news_text = nltk.Text(news_tokens)\n",
    "news_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "and\n",
      "are\n",
      "as\n",
      "at\n",
      "an\n",
      "also\n",
      "all\n",
      "about\n",
      "able\n",
      "around\n",
      "accept\n",
      "any\n",
      "according\n",
      "area\n",
      "always\n",
      "attackers\n",
      "against\n",
      "ability\n",
      "attempt\n",
      "another\n",
      "almost\n",
      "authors\n",
      "actually\n",
      "account\n",
      "arrangement\n",
      "available\n",
      "app\n",
      "above\n",
      "asks\n",
      "allocated\n",
      "away\n",
      "anything\n",
      "anyone\n",
      "assumption\n",
      "academics\n",
      "attack\n",
      "access\n",
      "applied\n",
      "act\n",
      "along\n",
      "active\n",
      "afford\n",
      "affordability\n",
      "adjust\n",
      "apps\n",
      "assess\n",
      "accordingly\n",
      "accord\n",
      "adopt\n",
      "agent\n",
      "attractive\n",
      "alone\n",
      "arrangements\n",
      "allow\n",
      "attacks\n",
      "agree\n",
      "analyst\n",
      "action\n",
      "again\n",
      "accident\n",
      "across\n",
      "about.\n",
      "abuse\n",
      "average\n",
      "additional\n",
      "administrators\n",
      "automated\n",
      "analysis\n",
      "allowing\n",
      "aware\n",
      "assemble\n",
      "adding\n",
      "attitude\n",
      "agreements\n",
      "android\n",
      "authority\n",
      "approach\n",
      "attracts\n",
      "agency\n",
      "attempts\n",
      "affect\n",
      "already\n",
      "ad\n",
      "added\n",
      "approaches\n",
      "applications\n",
      "applying\n",
      "alignment\n",
      "age\n",
      "abstract\n",
      "absolutely\n",
      "adds\n",
      "administered\n",
      "actions\n",
      "awareness\n",
      "activities\n",
      "altogether\n",
      "appeal\n",
      "averages\n",
      "addition\n",
      "accumulate\n",
      "assume\n",
      "annual\n",
      "amortization\n",
      "assuming\n",
      "adjustments\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"a\". The top ten words include a, and, are, as, at, an, also, all, about, able\n",
    "from nltk import FreqDist\n",
    "news_fdist = FreqDist(news_text)\n",
    "for w in news_fdist:\n",
    "    if w.startswith('a'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employment\n",
      "every\n",
      "enough\n",
      "expect\n",
      "employees\n",
      "end\n",
      "earn\n",
      "everything\n",
      "else\n",
      "example\n",
      "easier\n",
      "either\n",
      "each\n",
      "easy\n",
      "even\n",
      "encryption\n",
      "exercises\n",
      "empowered\n",
      "effort\n",
      "easily\n",
      "e-mail\n",
      "electricity\n",
      "economic\n",
      "entire\n",
      "ever\n",
      "error\n",
      "efforts\n",
      "everywhere\n",
      "ensures\n",
      "estimated\n",
      "estimates\n",
      "entitled\n",
      "enjoy\n",
      "equipment\n",
      "expends\n",
      "evolved\n",
      "economists\n",
      "efficiency\n",
      "equitable\n",
      "extended\n",
      "economically\n",
      "expense\n",
      "extensive\n",
      "elements\n",
      "exercises—like\n",
      "employers\n",
      "ends\n",
      "embarrassing\n",
      "elections\n",
      "exploits\n",
      "encrypt\n",
      "entirely\n",
      "extra\n",
      "expand\n",
      "eventually\n",
      "engineer\n",
      "estimate\n",
      "executable\n",
      "errors\n",
      "exploitation\n",
      "expects\n",
      "examined\n",
      "etiquette\n",
      "effects\n",
      "expert\n",
      "enforceable\n",
      "exert\n",
      "endangers\n",
      "espionage\n",
      "echelons\n",
      "enclaves\n",
      "exactly\n",
      "exploring\n",
      "epi-linked\n",
      "exposure\n",
      "encouraging\n",
      "expensive\n",
      "expected\n",
      "earner\n",
      "earning\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"e\". The top words include employment, every, enough, expect, employees, end, earn, everything, else\n",
    "example\n",
    "for w in news_fdist:\n",
    "    if w.startswith('e'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "is\n",
      "it\n",
      "income\n",
      "its\n",
      "into\n",
      "if\n",
      "instructions\n",
      "internet\n",
      "independent\n",
      "industry\n",
      "increasingly\n",
      "including\n",
      "impose\n",
      "incentives\n",
      "idea\n",
      "inside\n",
      "important\n",
      "improve\n",
      "interests\n",
      "insecurity\n",
      "interest\n",
      "insurance\n",
      "issue\n",
      "issued\n",
      "individuals\n",
      "independence\n",
      "interfere\n",
      "instead\n",
      "idle\n",
      "inexperienced\n",
      "increases\n",
      "illness\n",
      "informed\n",
      "interview\n",
      "indignity\n",
      "infrastructure\n",
      "inaccessible\n",
      "influence\n",
      "information\n",
      "interact\n",
      "implies\n",
      "instructions—for\n",
      "impossible\n",
      "innocent\n",
      "innocence\n",
      "innocuous-looking\n",
      "instil\n",
      "internet-connected\n",
      "issues\n",
      "in.\n",
      "involved\n",
      "impunity\n",
      "innovative\n",
      "items\n",
      "improvements\n",
      "improvement\n",
      "instrumental\n",
      "individual\n",
      "imposes\n",
      "instantiates\n",
      "image\n",
      "innovations\n",
      "insulin\n",
      "impossibility\n",
      "insurers\n",
      "innovation—defined\n",
      "incentive\n",
      "intensive\n",
      "identified\n",
      "isn\n",
      "interrupts\n",
      "illegal\n",
      "include\n",
      "in-progress\n",
      "indicator\n",
      "itself\n",
      "initial\n",
      "improved\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"i\". The top ten words include in, is, it, income, its, into, if, instructions, internet, independent\n",
    "for w in news_fdist:\n",
    "    if w.startswith('i'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "on\n",
      "or\n",
      "out\n",
      "one\n",
      "other\n",
      "only\n",
      "operating\n",
      "over\n",
      "often\n",
      "own\n",
      "ones\n",
      "offers\n",
      "offender\n",
      "outside\n",
      "opportunity\n",
      "otherwise\n",
      "operate\n",
      "online\n",
      "outbreaks\n",
      "obligations\n",
      "order\n",
      "options\n",
      "onto\n",
      "ought\n",
      "obviously\n",
      "owe\n",
      "owners\n",
      "outcome\n",
      "oil\n",
      "optional\n",
      "offline\n",
      "opportunities\n",
      "oft-cited\n",
      "offer\n",
      "outcomes\n",
      "overflow\n",
      "older\n",
      "originally\n",
      "overview\n",
      "operations\n",
      "operator\n",
      "outdated\n",
      "open-source\n",
      "obtain\n",
      "on—all\n",
      "old\n",
      "our\n",
      "outbreak\n",
      "offenders\n",
      "off\n",
      "opted\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"o\". The top ten words include of, on, or, out, one, other, only, operating, over, often\n",
    "for w in news_fdist:\n",
    "    if w.startswith('o'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used\n",
      "up\n",
      "using\n",
      "use\n",
      "users\n",
      "under\n",
      "usually\n",
      "unemployment\n",
      "unscramble\n",
      "until\n",
      "unreported\n",
      "unspotted\n",
      "unexpected\n",
      "unprotected\n",
      "upper\n",
      "usable\n",
      "unable\n",
      "ubiquity\n",
      "unavoidable\n",
      "unusually\n",
      "usual\n",
      "ubiquitous\n",
      "unchecked\n",
      "us\n",
      "ultimately…\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"u\". The top ten words include used, up, using, use, users, under, usually, unemployment,unscramble,until\n",
    "for w in news_fdist:\n",
    "    if w.startswith('u'):\n",
    "        print(w) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
