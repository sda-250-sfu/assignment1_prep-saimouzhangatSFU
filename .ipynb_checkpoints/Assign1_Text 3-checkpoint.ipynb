{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 3: News articles (from various news channels)\n",
    "\n",
    "Retrieved from:\n",
    "\n",
    "Free exchange: Apps and downsides. (2016 November 5). The Economist. Retrieved from \n",
    "https://www.economist.com/finance-and-economics/2016/11/05/apps-and-downsides?fsrc=sponsor%2Fopenandsecure\n",
    "Mangione, K. (2021 February 9). Here’s how much you’d have to earn to buy a house or condo \n",
    "in Vancouver, according to a study. CTV News. Retrieved from \n",
    "https://bc.ctvnews.ca/here-s-how-much-you-d-have-to-earn-to-buy-a-house-or-condo-in-vancouver-according-to-a-study-1.5302233\n",
    "Martins, N. (2021 February 9). B.C. records another 435 cases of COVID-19, four more deaths. \n",
    "News 1130. Retrieved from https://www.citynews1130.com/2021/02/09/bc-covid-435-cases-four-deaths-tuesday/\n",
    "Nicholson, B. (2021 February 9). Only 12% of COVID-19 fines paid in B.C. while majority \n",
    "dispute ticket. News 1130. Retrieved from https://www.citynews1130.com/2021/02/09/12-percent-covid-fines-paid-bc/\n",
    "Why everything is hackable: Computer security is broken from top to bottom. (2017 April 8). \n",
    "The Economist. Retrieved from https://www.economist.com/science-and-technology/2017/04/08/computer-security-is-broken-from-top-to-bottom?fsrc=sponsor%2Fopenandsecure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import corpus reader functionalities\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#Point to the path where you have some files\n",
    "#Change this for your own path\n",
    "corpus_root = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'ear_nose_throat_journal_clinicalstudies.txt',\n",
       " 'new_england_journal_of_medicine_perspectives.txt',\n",
       " 'news.txt']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = PlaintextCorpusReader(corpus_root, '.*')\n",
    "texts.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apps', 'and', 'downsides', '“', 'Gig', '-', ...]\n"
     ]
    }
   ],
   "source": [
    "# segment the text into words\n",
    "news_words = texts.words('news.txt')\n",
    "print(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6345"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of words\n",
    "len(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to calculate the lexical diversity\n",
    "def lexical_diversity(nameOfSource):\n",
    "    lexdiv= len(nameOfSource)/len(set(nameOfSource))\n",
    "    return lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.440889370932755"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the lexcial diversity \n",
    "lexical_diversity(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apps', 'and', 'downsides', '“', 'Gig', '-', 'economy', '”', 'work', 'sits', 'outside', 'normal', 'employment', 'categories', 'DURING', 'a', 'recent', 'ride', 'with', 'Uber', ',', 'this', 'passenger', 'received', 'a', 'surprising', 'word', 'of', 'thanks', 'for', 'talking', 'softly', '.'], ['To', 'complete', 'the', 'job', ',', 'the', 'driver', 'needed', 'to', 'follow', 'the', 'route', 'provided', 'by', 'Uber', ',', 'read', 'out', 'turn', '-', 'by', '-', 'turn', 'by', 'his', 'phone', ';', 'noise', 'from', 'the', 'back', 'seat', 'drowned', 'out', 'the', 'critical', 'instructions', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Segment the text into sentences \n",
    "news_sentences = texts.sents('news.txt')\n",
    "print(news_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# Count the length of longest sentence \n",
    "news__longest_sentence_length = max(len(s) for s in news_sentences)\n",
    "print(news__longest_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Kenneth', 'White', ',', 'a', 'cryptography', 'researcher', 'in', 'Washington', ',', 'DC', ',', 'warns', 'that', 'if', 'the', 'government', 'comes', 'down', 'too', 'hard', ',', 'the', 'software', 'business', 'may', 'end', 'up', 'looking', 'like', 'the', 'pharmaceutical', 'industry', ',', 'where', 'tough', ',', 'ubiquitous', 'regulation', 'is', 'one', 'reason', 'why', 'the', 'cost', 'of', 'developing', 'a', 'new', 'drug', 'is', 'now', 'close', 'to', 'a', 'billion', 'dollars', '(', '50', ').']]\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sentence \n",
    "news_longest_sentence = [s for s in news_sentences if len(s) == news__longest_sentence_length]\n",
    "print(news_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kenneth\n",
      "white\n",
      ",\n",
      "a\n",
      "cryptographi\n",
      "research\n",
      "in\n",
      "washington\n",
      ",\n",
      "DC\n",
      ",\n",
      "warn\n",
      "that\n",
      "if\n",
      "the\n",
      "govern\n",
      "come\n",
      "down\n",
      "too\n",
      "hard\n",
      ",\n",
      "the\n",
      "softwar\n",
      "busi\n",
      "may\n",
      "end\n",
      "up\n",
      "look\n",
      "like\n",
      "the\n",
      "pharmaceut\n",
      "industri\n",
      ",\n",
      "where\n",
      "tough\n",
      ",\n",
      "ubiquit\n",
      "regul\n",
      "is\n",
      "one\n",
      "reason\n",
      "whi\n",
      "the\n",
      "cost\n",
      "of\n",
      "develop\n",
      "a\n",
      "new\n",
      "drug\n",
      "is\n",
      "now\n",
      "close\n",
      "to\n",
      "a\n",
      "billion\n",
      "dollar\n",
      "(\n",
      "50\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stem the longest sentence \n",
    "news_longest_sentence_text = \"Kenneth White, a cryptography researcher in Washington, DC, warns that if the government comes down too hard, the software business may end up looking like the pharmaceutical industry, where tough, ubiquitous regulation is one reason why the cost of developing a new drug is now close to a billion dollars (50).\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "porter = PorterStemmer()\n",
    "news_longest_sentence_tokenized = word_tokenize(news_longest_sentence_text)\n",
    "for t in news_longest_sentence_tokenized:\n",
    "    print(porter.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per cent; operating system; household income; credit-card details;\n",
      "minimum wage; could expect; NEWS 1130; electricity meters; national\n",
      "level; median income; economic incentives; medical devices; outside\n",
      "normal; National Bank; light bulbs; control Uber; labour markets;\n",
      "hacker God; mortgage payment; would need\n"
     ]
    }
   ],
   "source": [
    "# find the top collocations \n",
    "filePath3 = \"./data/news.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath3, \"r\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    news_data = f.read()\n",
    "    \n",
    "news_tokens = nltk.word_tokenize(news_data)\n",
    "news_text = nltk.Text(news_tokens)\n",
    "news_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "and\n",
      "are\n",
      "as\n",
      "at\n",
      "an\n",
      "also\n",
      "all\n",
      "about\n",
      "able\n",
      "around\n",
      "accept\n",
      "any\n",
      "according\n",
      "area\n",
      "always\n",
      "attackers\n",
      "against\n",
      "ability\n",
      "attempt\n",
      "another\n",
      "almost\n",
      "authors\n",
      "actually\n",
      "account\n",
      "arrangement\n",
      "available\n",
      "app\n",
      "above\n",
      "asks\n",
      "allocated\n",
      "away\n",
      "anything\n",
      "anyone\n",
      "assumption\n",
      "academics\n",
      "attack\n",
      "access\n",
      "applied\n",
      "act\n",
      "along\n",
      "active\n",
      "afford\n",
      "affordability\n",
      "adjust\n",
      "apps\n",
      "assess\n",
      "accordingly\n",
      "accord\n",
      "adopt\n",
      "agent\n",
      "attractive\n",
      "alone\n",
      "arrangements\n",
      "allow\n",
      "attacks\n",
      "agree\n",
      "analyst\n",
      "action\n",
      "again\n",
      "accident\n",
      "across\n",
      "about.\n",
      "abuse\n",
      "average\n",
      "additional\n",
      "administrators\n",
      "automated\n",
      "analysis\n",
      "allowing\n",
      "aware\n",
      "assemble\n",
      "adding\n",
      "attitude\n",
      "agreements\n",
      "android\n",
      "authority\n",
      "approach\n",
      "attracts\n",
      "agency\n",
      "attempts\n",
      "affect\n",
      "already\n",
      "ad\n",
      "added\n",
      "approaches\n",
      "applications\n",
      "applying\n",
      "alignment\n",
      "age\n",
      "abstract\n",
      "absolutely\n",
      "adds\n",
      "administered\n",
      "actions\n",
      "awareness\n",
      "activities\n",
      "altogether\n",
      "appeal\n",
      "averages\n",
      "addition\n",
      "accumulate\n",
      "assume\n",
      "annual\n",
      "amortization\n",
      "assuming\n",
      "adjustments\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"a\". The top ten words include a, and, are, as, at, an, also, all, about, able\n",
    "from nltk import FreqDist\n",
    "news_fdist = FreqDist(news_text)\n",
    "for w in news_fdist:\n",
    "    if w.startswith('a'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employment\n",
      "every\n",
      "enough\n",
      "expect\n",
      "employees\n",
      "end\n",
      "earn\n",
      "everything\n",
      "else\n",
      "example\n",
      "easier\n",
      "either\n",
      "each\n",
      "easy\n",
      "even\n",
      "encryption\n",
      "exercises\n",
      "empowered\n",
      "effort\n",
      "easily\n",
      "e-mail\n",
      "electricity\n",
      "economic\n",
      "entire\n",
      "ever\n",
      "error\n",
      "efforts\n",
      "everywhere\n",
      "ensures\n",
      "estimated\n",
      "estimates\n",
      "entitled\n",
      "enjoy\n",
      "equipment\n",
      "expends\n",
      "evolved\n",
      "economists\n",
      "efficiency\n",
      "equitable\n",
      "extended\n",
      "economically\n",
      "expense\n",
      "extensive\n",
      "elements\n",
      "exercises—like\n",
      "employers\n",
      "ends\n",
      "embarrassing\n",
      "elections\n",
      "exploits\n",
      "encrypt\n",
      "entirely\n",
      "extra\n",
      "expand\n",
      "eventually\n",
      "engineer\n",
      "estimate\n",
      "executable\n",
      "errors\n",
      "exploitation\n",
      "expects\n",
      "examined\n",
      "etiquette\n",
      "effects\n",
      "expert\n",
      "enforceable\n",
      "exert\n",
      "endangers\n",
      "espionage\n",
      "echelons\n",
      "enclaves\n",
      "exactly\n",
      "exploring\n",
      "epi-linked\n",
      "exposure\n",
      "encouraging\n",
      "expensive\n",
      "expected\n",
      "earner\n",
      "earning\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"e\". The top words include employment, every, enough, expect, employees, end, earn, everything, else\n",
    "example\n",
    "for w in news_fdist:\n",
    "    if w.startswith('e'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "is\n",
      "it\n",
      "income\n",
      "its\n",
      "into\n",
      "if\n",
      "instructions\n",
      "internet\n",
      "independent\n",
      "industry\n",
      "increasingly\n",
      "including\n",
      "impose\n",
      "incentives\n",
      "idea\n",
      "inside\n",
      "important\n",
      "improve\n",
      "interests\n",
      "insecurity\n",
      "interest\n",
      "insurance\n",
      "issue\n",
      "issued\n",
      "individuals\n",
      "independence\n",
      "interfere\n",
      "instead\n",
      "idle\n",
      "inexperienced\n",
      "increases\n",
      "illness\n",
      "informed\n",
      "interview\n",
      "indignity\n",
      "infrastructure\n",
      "inaccessible\n",
      "influence\n",
      "information\n",
      "interact\n",
      "implies\n",
      "instructions—for\n",
      "impossible\n",
      "innocent\n",
      "innocence\n",
      "innocuous-looking\n",
      "instil\n",
      "internet-connected\n",
      "issues\n",
      "in.\n",
      "involved\n",
      "impunity\n",
      "innovative\n",
      "items\n",
      "improvements\n",
      "improvement\n",
      "instrumental\n",
      "individual\n",
      "imposes\n",
      "instantiates\n",
      "image\n",
      "innovations\n",
      "insulin\n",
      "impossibility\n",
      "insurers\n",
      "innovation—defined\n",
      "incentive\n",
      "intensive\n",
      "identified\n",
      "isn\n",
      "interrupts\n",
      "illegal\n",
      "include\n",
      "in-progress\n",
      "indicator\n",
      "itself\n",
      "initial\n",
      "improved\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"i\". The top ten words include in, is, it, income, its, into, if, instructions, internet, independent\n",
    "for w in news_fdist:\n",
    "    if w.startswith('i'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "on\n",
      "or\n",
      "out\n",
      "one\n",
      "other\n",
      "only\n",
      "operating\n",
      "over\n",
      "often\n",
      "own\n",
      "ones\n",
      "offers\n",
      "offender\n",
      "outside\n",
      "opportunity\n",
      "otherwise\n",
      "operate\n",
      "online\n",
      "outbreaks\n",
      "obligations\n",
      "order\n",
      "options\n",
      "onto\n",
      "ought\n",
      "obviously\n",
      "owe\n",
      "owners\n",
      "outcome\n",
      "oil\n",
      "optional\n",
      "offline\n",
      "opportunities\n",
      "oft-cited\n",
      "offer\n",
      "outcomes\n",
      "overflow\n",
      "older\n",
      "originally\n",
      "overview\n",
      "operations\n",
      "operator\n",
      "outdated\n",
      "open-source\n",
      "obtain\n",
      "on—all\n",
      "old\n",
      "our\n",
      "outbreak\n",
      "offenders\n",
      "off\n",
      "opted\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"o\". The top ten words include of, on, or, out, one, other, only, operating, over, often\n",
    "for w in news_fdist:\n",
    "    if w.startswith('o'):\n",
    "        print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used\n",
      "up\n",
      "using\n",
      "use\n",
      "users\n",
      "under\n",
      "usually\n",
      "unemployment\n",
      "unscramble\n",
      "until\n",
      "unreported\n",
      "unspotted\n",
      "unexpected\n",
      "unprotected\n",
      "upper\n",
      "usable\n",
      "unable\n",
      "ubiquity\n",
      "unavoidable\n",
      "unusually\n",
      "usual\n",
      "ubiquitous\n",
      "unchecked\n",
      "us\n",
      "ultimately…\n"
     ]
    }
   ],
   "source": [
    "# find top words starting with \"u\". The top ten words include used, up, using, use, users, under, usually, unemployment,unscramble,until\n",
    "for w in news_fdist:\n",
    "    if w.startswith('u'):\n",
    "        print(w) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
